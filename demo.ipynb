{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and data processing\n",
    "\n",
    "We are going to formulate the given problem as a binary classification task. Specifically, we will consider two possible scenarios for a given pair of text and image: \n",
    "1. They are duplicates, namely they refer to the same advertisement, whence we label it 1.\n",
    "2. They refer to different advertisements, in which case we will label it with 0.\n",
    "\n",
    "We process the given data in the following way:\n",
    "\n",
    "1. Iterate through the text file and the directory of images and save the id, cluster_id, title, and path of the image (path). As all the entries are duplicates, we also add the label 1. Now we have a standard dataset for a supervised ML task.\n",
    "2. In order to create a balanced dataset, we iterate through the created data frame, and for each positive example, we create a negative one by keeping the image of the former and selecting a title with a different cluster id.  \n",
    "3. We remove the cluster id field, as it is no longer needed. In addition, we remove duplicates based on the columns title and the path. The existence of such rows might lead us to overestimate test metrics if these rows end up in the training and the test set.\n",
    "4. Finally, we shuffle the data frame and a training, validation, and test split. The training set will be used to train our model, the validation one to keep track of the model's performance during training, and the test set to evaluate the generalization capacity of the trained classifier.\n",
    "\n",
    "This is done in the make_dataset module.\n",
    "\n",
    "Let's examine how does the dataset look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7201</td>\n",
       "      <td>images/774_7201_1925119171793825126.jpg</td>\n",
       "      <td>25 Terrifying Bridges You Need To See To Believe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4632</td>\n",
       "      <td>images/459_4632_656972811801994164.jpg</td>\n",
       "      <td>Which Travel Card Has The Most Valuable Miles?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12537</td>\n",
       "      <td>images/280_3012_3584825922501474949.jpg</td>\n",
       "      <td>Discounts Seniors Didnt Know They Could Get</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7449</td>\n",
       "      <td>images/806_7449_-4302125762809967385.jpg</td>\n",
       "      <td>Ordering Out Again? Aspire to Cook with Blue A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5928</td>\n",
       "      <td>images/608_5928_-1817387669840422148.jpg</td>\n",
       "      <td>27 Movie Goofs You Never Noticed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                      path   \n",
       "0   7201   images/774_7201_1925119171793825126.jpg  \\\n",
       "1   4632    images/459_4632_656972811801994164.jpg   \n",
       "2  12537   images/280_3012_3584825922501474949.jpg   \n",
       "3   7449  images/806_7449_-4302125762809967385.jpg   \n",
       "4   5928  images/608_5928_-1817387669840422148.jpg   \n",
       "\n",
       "                                               title  label  \n",
       "0   25 Terrifying Bridges You Need To See To Believe      1  \n",
       "1     Which Travel Card Has The Most Valuable Miles?      1  \n",
       "2        Discounts Seniors Didnt Know They Could Get      0  \n",
       "3  Ordering Out Again? Aspire to Cook with Blue A...      1  \n",
       "4                   27 Movie Goofs You Never Noticed      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"train_data.csv\").dropna()\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the training set is balanced. Otherwise, we might have to perform upsampling/downsampling or use a weighted loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    7612\n",
       "0    7608\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.label.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same has to be checked for the validation and test set, as otherwise we would not be estimating the generalization capacity of the model accurately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"val_data.csv\").dropna()\n",
    "test_data = pd.read_csv(\"test_data.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    959\n",
       "1    944\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.label.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    956\n",
       "0    945\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.label.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems to be in order, let's proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model specification\n",
    "\n",
    "We will proceed with an early-fusion architecture and create a simple model as follows:\n",
    "\n",
    "1. Given a sample, we will embed the image and the title. Then we will concatenate the embeddings and pass them to a standard feedforward neural network (NN).\n",
    "2. We will use a pre-trained model for the embeddings, whose parameters we will freeze, and only learn the parameters of the rest of the architecture. \n",
    "3. For the embeddings, the [CLIP](https://arxiv.org/abs/2103.00020) transformer is chosen. This is a very good choice for the task, as we can embed both visual and textual information into the same vector space. Its embeddings are learned in Siamese-network fashion, hence we expect images and titles of the same meaning to be close.\n",
    "\n",
    "A small demonstration on how to use CLIP for image search can be found [here](https://www.sbert.net/examples/applications/image-search/README.html).\n",
    "\n",
    "Let's start implementing this model. We will create the following classes:\n",
    "1. A custom PyTorhc dataset, which will serve directly the embedded text and image to the classifier.\n",
    "2. A module where we concatenate the visual and textual embeddings and outputs the predictions.\n",
    "3. The deduplication model, a PyTorch Lighting module, where we specify all the necessary methods for training, validation, testing and inference. \n",
    "\n",
    "Lighting has many advantages, as one does not have to write training loops and also automatically detects and deploys the training inference on GPUs.\n",
    "This is not the case in PyTorch in general. \n",
    "\n",
    "These classes can be found in the utils module. \n",
    "\n",
    "Let's first instantiate the sentence transformer and create a truncation function for preprocessing a title before we encode it with CLIP. This is needed, as the maximal number of tokens that is accepted by CLIP is 77."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = SentenceTransformer('clip-ViT-B-32')\n",
    "for param in CLIP.parameters():\n",
    "    param.requires_grad = False\n",
    "tokenizer = CLIP._first_module().processor.tokenizer\n",
    "\n",
    "\n",
    "def truncate_title(title, tokenizer):\n",
    "    \"\"\"\n",
    "    Truncate sentences that exceed the CLIP max token limit (77 tokens including the\n",
    "    starting and ending tokens).\n",
    "\n",
    "    Args:\n",
    "        title(string): The sentence to truncate.\n",
    "        tokenizer(CLIPTokenizer): Pretrained CLIP tokenizer.\n",
    "    Returns:\n",
    "        string: truncated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    cur_title = title\n",
    "    tokens = tokenizer.encode(cur_title)\n",
    "\n",
    "    if len(tokens) > tokenizer.model_max_length:\n",
    "        # Omit the first token, hence return 75 tokens\n",
    "        truncated_tokens = tokens[1:tokenizer.model_max_length - 1]\n",
    "        cur_title = tokenizer.decode(truncated_tokens)\n",
    "\n",
    "        # Recursive output, as the encode(decode()) could yield different result\n",
    "        return truncate_title(cur_title, tokenizer)\n",
    "\n",
    "    else:\n",
    "        return cur_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the custom dataset class.\n",
    "\n",
    "The class serves the model input as a dictionary, having the option for it to be without a label. This is useful for inference. In addition, there are functionalities for balancing the dataset (for training) and using a limited number of rows (for development)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DedupDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Preprocesses and serves\n",
    "    dict of multimodal tensors as model input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data,\n",
    "            image_directory,\n",
    "            visual_transform,\n",
    "            textual_transform,\n",
    "            limit_dev_set=None,\n",
    "            random_state=42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new DedupDataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Dataframe containing image paths and labels.\n",
    "            image_directory (str): Path to directory containing images.\n",
    "            visual_transform (callable): A function/transform that takes in an PIL image\n",
    "                and returns a transformed version of the image.\n",
    "            textual_transform (callable): A function/transform that takes in a string of text\n",
    "                and returns a transformed version of the text.\n",
    "            limit_dev_set (int, optional): If specified, limits the size of the dataset. Use during development.\n",
    "                Default is None.\n",
    "            random_state (int, optional): Seed for random operations.\n",
    "                Default is 42.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_frame = data\n",
    "        self.limit_dev_set = limit_dev_set\n",
    "        if self.limit_dev_set:\n",
    "            if self.data_frame.shape[0] > self.limit_dev_set:\n",
    "                self.data_frame = self.data_frame.sample(\n",
    "                    limit_dev_set, random_state=random_state\n",
    "                )\n",
    "        self.data_frame = self.data_frame.reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        self.visual_transform = visual_transform\n",
    "        self.textual_transform = textual_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "           Returns the number of samples in the dataset.\n",
    "           \"\"\"\n",
    "\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "           Returns the sample at the given index.\n",
    "\n",
    "           Args:\n",
    "               idx (int): Index of the sample to return.\n",
    "\n",
    "           Returns:\n",
    "               dict: A dictionary containing image, text, and label tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_id = self.data_frame.loc[idx, \"id\"]\n",
    "\n",
    "        image = Image.open(\n",
    "            self.data_frame.loc[idx, \"path\"]\n",
    "        ).convert(\"RGB\")\n",
    "        image = torch.Tensor(self.visual_transform.encode(image))\n",
    "        title = truncate_title(self.data_frame.loc[idx, \"title\"], tokenizer)\n",
    "\n",
    "        text = torch.Tensor(\n",
    "            self.textual_transform.encode(\n",
    "                title\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if \"label\" in self.data_frame.columns:\n",
    "            label = torch.Tensor(\n",
    "                [self.data_frame.loc[idx, \"label\"]]\n",
    "            ).long().squeeze()\n",
    "            sample = {\n",
    "                \"id\": image_id,\n",
    "                \"image\": image,\n",
    "                \"text\": text,\n",
    "                \"label\": label\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                \"id\": image_id,\n",
    "                \"image\": image,\n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the architecture of our model. This a standard fully-connected classifier, where in the forward method we concatenate the visual and textual embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenateLanguageAndVision(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_cl,\n",
    "            loss_function,\n",
    "            lang_module,\n",
    "            vis_module,\n",
    "            lang_feature_dim,\n",
    "            vis_feature_dim,\n",
    "            fusion_out_dim,\n",
    "            dropout_probability,\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a module that concatenates language and vision features, fuses them, and outputs the predictions.\n",
    "\n",
    "        Args:\n",
    "            num_cl (int): The number of output classes.\n",
    "            loss_function (function): The loss function to use for training.\n",
    "            lang_module (torch.nn.Module): The module for processing language inputs.\n",
    "            vis_module (torch.nn.Module): The module for processing visual inputs.\n",
    "            lang_feature_dim (int): The number of output features from the language module.\n",
    "            vis_feature_dim (int): The number of output features from the vision module.\n",
    "            fusion_out_dim (int): The number of output features after fusing the language and vision features.\n",
    "            dropout_probability (float): The probability of dropping out a neuron during training.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(ConcatenateLanguageAndVision, self).__init__()\n",
    "        self.language_module = lang_module\n",
    "        self.vision_module = vis_module\n",
    "        self.fusion = torch.nn.Linear(\n",
    "            in_features=(lang_feature_dim + vis_feature_dim),\n",
    "            out_features=fusion_out_dim\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_out_dim,\n",
    "            out_features=num_cl\n",
    "        )\n",
    "        self.loss_function = loss_function\n",
    "        self.dropout = torch.nn.Dropout(dropout_probability)\n",
    "\n",
    "    def forward(self, text_inputs, image_inputs, labels=None):\n",
    "        \"\"\"\n",
    "        Concatenates language and vision features, fuses them, and returns the predictions.\n",
    "\n",
    "        Args:\n",
    "            text_inputs (torch.Tensor): The input text.\n",
    "            image_inputs (torch.Tensor): The input images.\n",
    "            labels (torch.Tensor): The target labels.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the predictions and the loss (if labels are provided).\n",
    "        \"\"\"\n",
    "        language_features = torch.nn.functional.relu(\n",
    "            self.language_module(text_inputs)\n",
    "        )\n",
    "        vision_features = torch.nn.functional.relu(\n",
    "            self.vision_module(image_inputs)\n",
    "        )\n",
    "        combined_features = torch.cat(\n",
    "            [language_features, vision_features], dim=1\n",
    "        )\n",
    "        fused_features = self.dropout(\n",
    "            torch.nn.functional.relu(\n",
    "                self.fusion(combined_features)\n",
    "            )\n",
    "        )\n",
    "        logits = self.fc(fused_features)\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=1)\n",
    "        loss = (\n",
    "            self.loss_function(predictions, labels)\n",
    "            if labels is not None else labels\n",
    "        )\n",
    "        return (predictions, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the Ligiting module, putting all the pieces together and creating methods for testing and inference.\n",
    "\n",
    "In order to account for overfitiing, we chosse the Adam optimizer with weight decay and also place a dropout layer after the fusion of the textual/visual features.\n",
    "\n",
    "We also add an early-stopping (maximum patience is 3 epochs) and a checkpoint callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DedupModel(pl.LightningModule):\n",
    "    def __init__(self, hyperparams):\n",
    "        \"\"\"\n",
    "        Initializes the DedupModel.\n",
    "\n",
    "        Args:\n",
    "            hyperparams (Namespace): Namespace containing the hyperparameters.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DedupModel, self).__init__()\n",
    "        self.params = hyperparams\n",
    "\n",
    "        self.embedding_dim = self.params.get(\"embedding_dim\", 512)\n",
    "        self.lang_feature_dim = self.params.get(\n",
    "            \"lang_feature_dim\", 300\n",
    "        )\n",
    "        self.vis_feature_dim = self.params.get(\n",
    "            # balance language and vision features by default\n",
    "            \"vis_feature_dim\", self.lang_feature_dim\n",
    "        )\n",
    "        self.out_path = Path(\n",
    "            self.params.get(\"out_path\", \"model_outputs\")\n",
    "        )\n",
    "\n",
    "        self.textual_transform = self._create_textual_transform()\n",
    "        self.visual_transform = self._create_visual_transform()\n",
    "        self.train_dataset = self._create_dataset(\"train_data\")\n",
    "        self.val_dataset = self._create_dataset(\"val_data\")\n",
    "\n",
    "        # set up model and training\n",
    "        self.model = self._create_model()\n",
    "        self.trainer_params = self._set_trainer_params()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    # Required Lightning Methods\n",
    "\n",
    "    def forward(self, text, image, label=None):\n",
    "        return self.model(text, image, label)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        preds, loss = self.forward(\n",
    "            text=batch[\"text\"],\n",
    "            image=batch[\"image\"],\n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        preds, loss = self.eval().forward(\n",
    "            text=batch[\"text\"],\n",
    "            image=batch[\"image\"],\n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        self.log(\"avg_val_loss\", loss, prog_bar=True)\n",
    "        return {\"batch_val_loss\": loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        avg_loss = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.validation_step_outputs.clear()\n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.params.get(\"lr\", 0.001)\n",
    "        )\n",
    "\n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer),\n",
    "                \"monitor\": \"avg_val_loss\"}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.params.get(\"batch_size\", 4),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            shuffle=False,\n",
    "            batch_size=self.params.get(\"batch_size\", 4),\n",
    "        )\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        self._set_seed(self.params.get(\"random_state\", 42))\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)\n",
    "\n",
    "    def _set_seed(self, seed):\n",
    "        \"\"\"\n",
    "        Sets the random seed for reproducibility.\n",
    "\n",
    "        Args:\n",
    "            seed (int): The random seed value.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _create_textual_transform(self):\n",
    "        \"\"\"\n",
    "        Creates and returns the textual transform for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            (SentenceTransformer): The textual transform.\n",
    "        \"\"\"\n",
    "\n",
    "        language_transform = CLIP\n",
    "        return language_transform\n",
    "\n",
    "    def _create_visual_transform(self):\n",
    "        \"\"\"\n",
    "        Creates and returns the visual transform for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            (torchvision.transforms.Compose): The visual transform.\n",
    "        \"\"\"\n",
    "        visual_transform = CLIP\n",
    "        return visual_transform\n",
    "\n",
    "    def _create_dataset(self, dataset_key):\n",
    "        \"\"\"\n",
    "        Creates and returns the dataset for the given dataset key.\n",
    "\n",
    "        Args:\n",
    "            dataset_key (str): The key of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            (DedupDataset): The dataset.\n",
    "        \"\"\"\n",
    "        return DedupDataset(\n",
    "            data=self.params[dataset_key],\n",
    "            image_directory=self.params.get(\"img_dir\"),\n",
    "            visual_transform=self.visual_transform,\n",
    "            textual_transform=self.textual_transform,\n",
    "            # limit training samples only\n",
    "            limit_dev_set=(\n",
    "                self.params.get(\"dev_limit\", None)\n",
    "                if \"train\" in str(dataset_key) else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"\n",
    "        Creates the model.\n",
    "\n",
    "        Returns:\n",
    "            (ConcatenateLanguageAndVision): Model.\n",
    "        \"\"\"\n",
    "\n",
    "        language_module = torch.nn.Linear(\n",
    "            in_features=self.embedding_dim,\n",
    "            out_features=self.lang_feature_dim\n",
    "        )\n",
    "\n",
    "        vision_module = torch.nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=self.vis_feature_dim\n",
    "        )\n",
    "\n",
    "        return ConcatenateLanguageAndVision(\n",
    "            num_cl=self.params.get(\"num_classes\", 2),\n",
    "            loss_function=torch.nn.CrossEntropyLoss(),\n",
    "            lang_module=language_module,\n",
    "            vis_module=vision_module,\n",
    "            lang_feature_dim=self.lang_feature_dim,\n",
    "            vis_feature_dim=self.vis_feature_dim,\n",
    "            fusion_out_dim=self.params.get(\n",
    "                \"fusion_output_size\", 512\n",
    "            ),\n",
    "            dropout_probability=self.params.get(\"dropout_p\", 0.1),\n",
    "        )\n",
    "\n",
    "    def _set_trainer_params(self):\n",
    "        \"\"\"\n",
    "        Sets the parameters for the model trainer.\n",
    "\n",
    "        Returns:\n",
    "        Dict: Dictionary containing the parameters for the model trainer.\n",
    "        \"\"\"\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=self.out_path,\n",
    "            monitor=self.params.get(\n",
    "                \"checkpoint_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            mode=self.params.get(\n",
    "                \"checkpoint_monitor_mode\", \"min\"\n",
    "            ),\n",
    "            verbose=self.params.get(\"verbose\", True),\n",
    "            save_top_k=1\n",
    "\n",
    "        )\n",
    "\n",
    "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "            monitor=self.params.get(\n",
    "                \"early_stop_monitor\", \"avg_val_loss\"\n",
    "            ),\n",
    "            min_delta=self.params.get(\n",
    "                \"early_stop_min_delta\", 0.001\n",
    "            ),\n",
    "            patience=self.params.get(\n",
    "                \"early_stop_patience\", 3\n",
    "            ),\n",
    "            verbose=self.params.get(\"verbose\", True),\n",
    "        )\n",
    "\n",
    "        trainer_params = {\n",
    "            \"callbacks\": [early_stop_callback, checkpoint_callback],\n",
    "            \"default_root_dir\": self.out_path,\n",
    "            \"accumulate_grad_batches\": self.params.get(\n",
    "                \"accumulate_grad_batches\", 1\n",
    "            ),\n",
    "            \"num_nodes\": self.params.get(\"n_gpu\", 1),\n",
    "            \"max_epochs\": self.params.get(\"max_epochs\", 100),\n",
    "            \"gradient_clip_val\": self.params.get(\n",
    "                \"gradient_clip_value\", 1\n",
    "            ),\n",
    "        }\n",
    "        return trainer_params\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_metrics(self):\n",
    "        \"\"\"\n",
    "       Computes the Area Under the Curve (AUC) for the Receiver Operator Curve.\n",
    "       Computes the F1 score for the classifier.\n",
    "\n",
    "       Returns:\n",
    "       float: The AUC on the test set.\n",
    "       float: The F1 score on the test set.\n",
    "        \"\"\"\n",
    "        test_dataset = self._create_dataset(\"test_data\")\n",
    "        test_predictions = pd.DataFrame(\n",
    "            index=test_dataset.data_frame.id,\n",
    "            columns=[\"proba\", \"label\"]\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            shuffle=False,\n",
    "            batch_size=self.params.get(\"batch_size\", 4))\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            preds, _ = self.model.eval()(\n",
    "                batch[\"text\"], batch[\"image\"]\n",
    "            )\n",
    "            test_predictions.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
    "            test_predictions.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
    "        test_predictions.proba = test_predictions.proba.astype(float)\n",
    "        test_predictions.label = test_predictions.label.astype(int)\n",
    "        return roc_auc_score(test_dataset.data_frame.label, test_predictions.proba), \\\n",
    "            f1_score(test_dataset.data_frame.label, test_predictions.label)\n",
    "\n",
    "    def inference_sample(self, unseen_img_path, unseen_title):\n",
    "        \"\"\"\n",
    "        This function takes a pre-trained CLIP model and uses it to make predictions on an unseen image and title, after\n",
    "        preprocessing the title using a tokenizer and truncating it to a maximum length.\n",
    "\n",
    "        Args:\n",
    "            model: A pre-trained DedupModel model.\n",
    "            unseen_img_path (str): The path to an unseen image.\n",
    "            unseen_title (str): The title or description of the image.\n",
    "\n",
    "        Returns:\n",
    "            prediction (int): The predicted label index for the image.\n",
    "\n",
    "        \"\"\"\n",
    "        title_embedding = torch.unsqueeze(torch.Tensor(CLIP.encode(truncate_title(unseen_title, tokenizer))), 0)\n",
    "        img_embedding = torch.unsqueeze(torch.Tensor(CLIP.encode(Image.open(unseen_img_path).convert(\"RGB\"))), 0)\n",
    "        preds, _ = self.model.eval()(title_embedding, img_embedding)\n",
    "        prediction = int(preds.argmax(dim=1).item())\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained and evaluated in the main module. If you wish you ca run the following cell and evaluate it. \n",
    "\n",
    "In the sake of keeping the output of the report simple, we will just load it from its best checkpoint and evaluate its performance.\n",
    "\n",
    "In model_outputs, one can find the mentioned checkpoint and the metrics.csv, which kept track of the model validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd()\n",
    "img_path = data_dir / \"images\"\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"train_data\": train_data,\n",
    "    \"val_data\": val_data,\n",
    "    \"test_data\": test_data,\n",
    "    \"img_dir\": img_path,\n",
    "    \"embedding_dim\": 512,\n",
    "    \"lang_feature_dim\": 500,\n",
    "    \"vis_feature_dim\": 600,\n",
    "    \"fusion_output_size\": 256,\n",
    "    \"out_path\": \"model_outputs\",\n",
    "    \"dev_limit\": None,\n",
    "    \"lr\": 0.0005,\n",
    "    \"max_epochs\": 12,\n",
    "    \"n_gpu\": 0,\n",
    "    \"batch_size\": 64,\n",
    "    \"accumulate_grad_batches\": 16,\n",
    "    \"early_stop_patience\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "# dedup_model = DedupModel(hyperparams=hyperparams)\n",
    "# dedup_model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████▊                               | 19/30 [01:27<00:50,  4.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (91 > 77). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 30/30 [02:16<00:00,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ROC AUC score: 0.9203969360873128 and test F1 score is: 0.8700846192135391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoints = list(Path(\"model_outputs\").glob(\"*.ckpt\"))\n",
    "\n",
    "dedup_model = DedupModel.load_from_checkpoint(\n",
    "    checkpoints[0],\n",
    "    hyperparams=hyperparams\n",
    ")\n",
    "test_roc, f1 = dedup_model.test_metrics()\n",
    "print(f\"test ROC AUC score: {test_roc} and test F1 score is: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model performs quite well, as the AUC of the ROC for the trained classifier and the F1 score are quite high, meaning that the model has quite high and low percentages of True Positives and False Positives, respectively, for different classification thresholds, as well as that it achieves high precision and recall.\n",
    "\n",
    "Attention is indeed all you need :)\n",
    "\n",
    "This could be improved potentially if:\n",
    "\n",
    "1. We chose a deeper and/or wider architecture for the network.\n",
    "2. We fine-tuned the transformer. This is computationally expensive though, as it has 151 M parameters.\n",
    "3. Use the multimodal information differently, i.e. choose another fusion strategy.\n",
    "\n",
    "The module inference can be used to carry out a prediction an unseen title, image pair. Finally, we have implemented a sample unit test for the custom dataset class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
